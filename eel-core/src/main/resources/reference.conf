eel {

  components = [
    {
      namespace = "parquet"
      impl = "io.eels.component.parquet.ParquetComponent"
    }
    {
      namespace = "csv"
      impl = "io.eels.component.csv.CsvComponent"
    }
    {
      namespace = "jdbc"
      impl = "io.eels.component.jdbc.JdbcComponent"
    }
    {
      namespace = "hive"
      impl = "io.eels.component.hive.HiveComponent"
    }
  ]

  # the default size of queues used by streams
  default-buffer-size = 100

  # the size of each chunk from a source
  default-batch-size = 100

  csv {
    skipBadRows = false
  }

  jdbc {
    oracle {
      # used when precision is 0 for decimal types
      default-precision = 38
      # used when scale is -127 for decimal types
      default-scale = 20
    }
  }

  avro {
    fillMissingValues = false
    caseSensitive = true
    java.string = true
    deserializeAsNullable = false
  }

  parquet {
    mergeSchemas = true
    parallelism = 5
    maxRecordsPerFile = 0
    maxFileSize = 0
    skipCrc = false
    compressionCodec = "snappy"
    caseSensitive = true
  }

  sqlContext {
    ignoreCase = true
    writeToDisk = true
    dataDirectory = "~"
  }

  execution {
    requestSize = 100
    timeout = 1d
  }

  plans {
    counts {
      distinctValueCap = 100
    }
  }

  jdbc {
    sink {
      bufferSize = 10000
      autoCommit = false
      warnIfMissingRewriteBatchedStatements = true
      swallowExceptions = false
    }
  }
}